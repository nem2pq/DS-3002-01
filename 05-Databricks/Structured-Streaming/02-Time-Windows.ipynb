{"cells":[{"cell_type":"markdown","source":["### Working with Time Windows\n\n#### Objectives:\n* Use sliding windows to aggregate over chunks of data rather than all data\n* Apply watermarking to throw away stale old data that you do not have space to keep\n* Plot live graphs using `display`\n\nFirst, run the following cell to import the data and make various utilities available for our experimentation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acd38de2-2b77-41e9-a590-3605fe30b697"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42835eca-233b-4a99-9965-7386d973c790"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Streaming Aggregations\nContinuous applications often require near real-time decisions on real-time, aggregated statistics.\n\nSome examples include: \n* Aggregating errors by type in data from IoT devices. \n* Detecting anomalous behavior by aggregating data by country from a server's log file(s). \n* Performing behavioral analysis on instant messages via hash tags.\n\nHowever, in the case of streams, you generally don't want to run aggregations over the entire dataset. \n<br>\n\n**But what happens if an attempt is made to aggregate over a stream's entire dataset?** <br>\n- While streams have a definitive start, conceptually there's no end to a stream; i.e., it's an [unbounded] data set.\n- Because there is no \"end\" to a stream, the size of the dataset grows in perpetuity.\n- Which means that your cluster would eventually run out of resources.\n\nSo, instead of aggregating over an entire dataset, streaming data must be aggregated over by grouping the data by *windows* of time (e.g.,, every 5 minutes, or every hour).  This technical approach is referred to as **windowing**.\n\n<br>\n\n### Windowing\nIf we were using a static DataFrame to produce an aggregate count, we could use `groupBy()` and `count()`.  However, we must insteadn accumulate counts within a **sliding window** in order to answer questions such as *\"How many records are we getting every second?\"*\n\nThe following illustration, from the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\" target=\"_blank\">Structured Streaming Programming Guide</a> guide, helps us understanding how it works:\n\n<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-window.png\" style=\"width: 900px;\">\n\n<br>\n\n#### Event Time *versus* Receipt Time\n- **Event Time** is the time at which the event occurred in the real world.\n- **Event Time** is **NOT** maintained by the Structured Streaming framework. \n\nAt best, Streams only knows about **Receipt Time** - the time a piece of data arrived in Spark.\n\n##### Examples of *Event Time*:\n* The timestamp recorded in each record of a log file\n* The instant at which an IoT device took a measurement\n* The moment a REST API received a request\n\n##### Examples of Receipt Time:\n- A timestamp added to a DataFrame the moment it was processed by Spark\n- The timestamp extracted from an hourly log file's file name\n- The time at which an IoT hub received a report of a device's measurement\n- Presumably offset by some delay from when the measurement was taken\n\nHowever, it should be born in mind that there are some problems inherent to using the **Receipt Time...** the main problem pertaining to accuracy.\n\nFor example, the time between when an IoT device takes a measurement versus when it is reported can be off by several minutes.  This may be of significant concern with regards to security and health devices. For example:\n- The timestamp embedded in an hourly log file can be off by up to one hour making correlations to other events extremely difficult\n- The timestamp added by Spark as part of a DataFrame transformation can be off by hours to weeks to months depending on when the event occurred and when the job ran\n\nSo then, are there situations where using **Receipt Time** rather than **Event Time** may be more appropriate?  Well, it depends... Receipt Time could be appropriate in circumstances where accuracy is not a significant concern (i.e., when **Receipt Time** is close enough to **Event Time**).\nFor example, where IoT events that could be delayed by minutes, but where the resolution of the query is greater (e.g., days or months)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51fc179f-ce3e-4346-9519-4669ce2c5a35"}}},{"cell_type":"markdown","source":["#### 1.0. Windowed Streaming\nThis exercise examines the files in `/mnt/training/sensor-data/accelerometer/time-series-stream.json/`.  Each line in this file contains a JSON record having two fields: `time` and `action`. Consider that new files will be written to this directory continuously (aka, streaming); therefore, conceptually there is no end to this process.\n\n###### 1.1. First, inspect the `head` of one of these files:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04ff38a7-1cfb-401f-a6da-7f6530783b69"}}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/sensor-data/accelerometer/time-series-stream.json/file-0.json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"866e16d9-ec88-412d-a823-369064ec1338"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###### 1.2. Next, analyze these files interactively. \nA schema must be specified for file-based Structured Streams; therefore, the first task is to define a schema for these files.\nDue to its simplicity, the schema can be defined using a simple DDL-formatted string representation of the schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2cfabd1-b916-471d-8aba-5abc36ecd456"}}},{"cell_type":"code","source":["inputPath = \"dbfs:/mnt/training/sensor-data/accelerometer/time-series-stream.json/\"\njsonSchema = \"time timestamp, action string\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0acbbead-7fbe-4cb2-ace6-f410cf4efd91"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###### 1.3. With the schema defined, create the initial DataFrame `inputDf`, and then `countsDF` to represent the aggregation:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7ef3ca4-4a73-42c8-8dc2-71d8da6abbfc"}}},{"cell_type":"code","source":["from pyspark.sql.functions import window, col\n\ninputDF = (spark\n  .readStream                                 # Returns an instance of DataStreamReader\n  .schema(jsonSchema)                         # Set the schema of the JSON data\n  .option(\"maxFilesPerTrigger\", 1)            # Treat a sequence of files as a stream, one file at a time\n  .json(inputPath)                            # Specifies the format, path and returns a DataFrame\n)\n\ncountsDF = (inputDF\n  .groupBy(col(\"action\"),                     # Aggregate by action...\n           window(col(\"time\"), \"1 hour\"))     # ...then by a 1 hour window\n  .count()                                    # For the aggregate, produce a count\n  .select(col(\"window.start\").alias(\"start\"), # Elevate field to column\n          col(\"count\"),                       # Include count\n          col(\"action\"))                      # Include action\n  .orderBy(col(\"start\"))                      # Sort by the start time\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a465409-33fc-4097-910c-bfa325b5c327"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###### 1.4. To view the results of the query, pass the DataFrame `countsDF` to the `display()` function.\nAs in the previous lesson, specify the stream's `streamName` to gain better control of it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b3158d9-42dd-4845-b4b2-786c6c1092d0"}}},{"cell_type":"code","source":["streamName = \"lesson03_ps\"\ndisplay(countsDF,  streamName = streamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d184df8-880a-4d49-9391-015be804ef7a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 2.0. Performance Considerations\nIf you run that query, as is, it will take a surprisingly long time to start generating data. What's the cause of the delay? If you expand the **Spark Jobs** component, you'll see something like this:\n\n<img src=\"https://files.training.databricks.com/images/structured-streaming-shuffle-partitions-200.png\"/>\n\n<br>\n\nIt's our `groupBy()`. `groupBy()` causes a _shuffle_, and, by default, Spark SQL shuffles to 200 partitions. In addition, we're doing a _stateful_ aggregation: one that requires Structured Streaming to maintain and aggregate data over time.\n\nWhen doing a stateful aggregation, Structured Streaming must maintain an in-memory _state map_ for each window within each partition. For fault tolerance reasons, the state map has to be saved after a partition is processed, and it needs to be saved somewhere fault-tolerant. To meet those requirements, the Streaming API saves the maps to a distributed store. On some clusters, that will be HDFS. Databricks uses the DBFS.\n\nThat means that every time it finishes processing a window, the Streaming API writes its internal map to disk. The write has some overhead, typically between 1 and 2 seconds."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7b25017-a7a2-456c-bd88-2d25b6822941"}}},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2bbbda4-4b75-4a9f-a82c-fd8481bd211b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Before proceeding, we need to stop any streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4c85100-13af-4aee-a1d2-fe3c466436ee"}}},{"cell_type":"code","source":["# for s in spark.streams.active: # Iterate over all active streams\n#   s.stop()                     # Stop the stream\n\n# As mentioned in lesson #2, we have provided additional methods for working with streams, and in  \n# this case, for dealing with the rare exceptions that may arise as a result of terminating a stream.\n# Listed above is the logical equivalent to this operation.\nstopAllStreams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20075051-7521-4aab-a0fb-80e6a525bf79"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["One way to reduce this overhead is to reduce the number of partitions Spark shuffles to. In most cases, you want a 1-to-1 mapping of partitions to cores for streaming applications.  Rerun the query below and notice the performance improvement.\n\nOnce the data is loaded, render a line graph with \n* **Keys** is set to `start`\n* **Series groupings** is set to `action`\n* **Values** is set to `count`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"984dd4f4-d367-4d68-82b7-37d6cc66489b"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\ndisplay(countsDF,  streamName = myStreamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5469f0de-8e1b-45ac-b350-1449fabb29f7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aa2d6c0-d8fa-4559-90b1-debd03ff0ff7"}}},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96192d89-0381-4295-a8f7-cf7fa2e00e99"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When you are done, stop all the streaming jobs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0518cf3-44ae-4f15-883e-4ed97c3804e1"}}},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fa12dff-6f5a-4f64-a613-f606eee65ea3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 3.0. Problem with Generating Many Windows\nWe are generating a window for every 1 hour aggregate.  _Every window_ has to be separately persisted and maintained. Over time, this aggregated data will build up in the driver. The end result being a massive slowdown if not an OOM Error.\n\n###### How do we fix that problem?\nOne simple solution is to increase the size of our window (say, to 2 hours). That way, we're generating fewer windows.\nBut if the job runs for a long time, we're still building up an unbounded set of windows. Eventually, we could hit resource limits.\n\n<br>\n\n#### 3.1. Watermarking\nA better solution to the problem is to define a cut-off (i.e., a point after which Structured Streaming is allowed to throw saved windows away). That's what _watermarking_ allows us to do.  **Refining our previous example:** Below is our previous example with watermarking. We're telling Structured Streaming to keep no more than 2 hours of aggregated data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"270a342e-b747-4d00-be18-9212e4033d56"}}},{"cell_type":"code","source":["watermarkedDF = (inputDF\n  .withWatermark(\"time\", \"2 hours\")             # Specify a 2-hour watermark\n  .groupBy(col(\"action\"),                       # Aggregate by action...\n           window(col(\"time\"), \"1 hour\"))       # ...then by a 1 hour window\n  .count()                                      # For each aggregate, produce a count\n  .select(col(\"window.start\").alias(\"start\"),   # Elevate field to column\n          col(\"count\"),                         # Include count\n          col(\"action\"))                        # Include action\n  .orderBy(col(\"start\"))                        # Sort by the start time\n)\ndisplay(watermarkedDF, streamName = myStreamName) # Start the stream and display it"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21f41e02-6c60-4a26-953f-80c360d4b8e7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the example above,   \n* Data received 2 hour _past_ the watermark will be dropped. \n* Data received within 2 hours of the watermark will never be dropped.\n\nMore specifically, any data less than 2 hours behind the latest data processed till then is guaranteed to be aggregated. However, the guarantee is strict only in one direction. Data delayed by more than 2 hours is not guaranteed to be dropped; it may or may not get aggregated. The more delayed the data is, the less likely the engine is going to process it.\n\n**Wait until stream is done initializing...**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4755fbda-fdbb-4876-90bd-fe576fcc7c4f"}}},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8ed51aa-201b-4ce1-9c9a-a3eb567c2c61"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Stop all the streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c6af38a-1784-4ccd-99e5-8a3e2bca6f32"}}},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03dc65f5-668e-4ab0-ac34-2d8e641d6539"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2372d763-10d8-4a17-a7e4-3e760c8ea2b9"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6864413d-2911-425b-80f2-75a2d9c873e5"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02-Time-Windows","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1430281655517795}},"nbformat":4,"nbformat_minor":0}
