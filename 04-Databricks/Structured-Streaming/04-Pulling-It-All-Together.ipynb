{"cells":[{"cell_type":"markdown","source":["### Gaining Actionable Insights from Twitter Data\nIn this capstone project, we'll implement Structured Streaming to gain insight from streaming Twitter data. The executive team would like to have access to some key business metrics such as:\n* The most tweeted hashtag in last 5 minute window\n* A map that identifies from where the tweets are coming\n\nFirst, run the following cell to import the data and make various utilities available for our experimentation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59e3bea7-d7b4-4add-b044-69bfe3b97659"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de243e27-48c4-4f8d-a514-dad062009c33"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 1.0. Read Streaming Data from Input Source\nThe input source is a a Kafka feed of Twitter data\n\nFor this step you will need to:\n0. Use the `format()` operation to specify \"kafka\" as the type of the stream\n0. Specify the location of the Kafka server by setting the option \"kafka.bootstrap.servers\" with one of the following values (depending on where you are located): \n * **server1.databricks.training:9092** (US-Oregon)\n * **server2.databricks.training:9092** (Singapore)\n0. Indicate which topics to listen to by setting the option \"subscribe\" to \"tweets\"\n0. Throttle Kafka's processing of the streams\n0. Rewind stream to beginning when we restart notebook\n0. Load the input data stream in as a DataFrame\n0. Select the column `value` - cast it to a `STRING`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ace33bf2-72e1-4a8f-b548-61f364e1b3ee"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\nkafkaServer = \"server1.databricks.training:9092\"   # US (Oregon)\n# kafkaServer = \"server2.databricks.training:9092\" # Singapore\n\nrawDF = (spark.readStream\n  .format(\"kafka\")                                       # Specify \"kafka\" as the type of the stream\n  .option(\"kafka.bootstrap.servers\", kafkaServer)        # Set the location of the kafka server\n  .option(\"subscribe\", \"tweets\")                         # Indicate which topics to listen to\n  .option(\"maxOffsetsPerTrigger\", 1000)                  # Throttle Kafka's processing of the streams\n  .option(\"startingOffsets\", \"earliest\")                 # Rewind stream to beginning when we restart notebook\n  .load()                                                # Load the input data stream in as a DataFrame\n  .select(col(\"value\").cast(\"STRING\"))                   # Select the \"value\" column and cast to a string\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1acf6e5-4d41-4a6d-a9f1-d94c0d019e85"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 2.0. Define a Schema for Parsing the JSON\nSimply run the following cell and proceed to the next step."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c370876-9f2f-4298-9b2e-64afe6b35f15"}}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, ArrayType\n\ntwitSchema = StructType([\n  StructField(\"hashTags\", ArrayType(StringType(), False), True),\n  StructField(\"text\", StringType(), True),   \n  StructField(\"userScreenName\", StringType(), True),\n  StructField(\"id\", LongType(), True),\n  StructField(\"createdAt\", LongType(), True),\n  StructField(\"retweetCount\", IntegerType(), True),\n  StructField(\"lang\", StringType(), True),\n  StructField(\"favoriteCount\", IntegerType(), True),\n  StructField(\"user\", StringType(), True),\n  StructField(\"place\", StructType([\n    StructField(\"coordinates\", StringType(), True), \n    StructField(\"name\", StringType(), True),\n    StructField(\"placeType\", StringType(), True),\n    StructField(\"fullName\", StringType(), True),\n    StructField(\"countryCode\", StringType(), True)]), \n  True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a1343c4-fb6b-4808-ab7c-fd2c12d86a89"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 3.0. Create a JSON DataFrame\nFrom the `rawDF` parse out the json subfields using `from_json`. Create a DataFrame that has fields\n* `time`\n* `json`, a nested field that has all the rest of the data\n* promote all `json` subfields to fields."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ef8b572-0b9a-4fae-9d5e-79f2513c7288"}}},{"cell_type":"code","source":["from pyspark.sql.functions import from_json, expr, col\n\ncleanDF = (rawDF\n  .withColumn(\"json\", from_json(col(\"value\"), twitSchema))                      # Add the column \"json\" by parsing the column \"value\" with \"from_json\"\n  .select(\n    expr(\"cast(cast(json.createdAt as double)/1000 as timestamp) as time\"),     # Cast \"createdAt\" column properly, call it \"time\"\n    col(\"json.hashTags\").alias(\"hashTags\"),                                     # Promote subfields of \"json\" column e.g. \"json.field\" to \"field\"\n    col(\"json.text\").alias(\"text\"),                                             # Repeat for each subfields of \"json\"\n    col(\"json.userScreenName\").alias(\"userScreenName\"),\n    col(\"json.id\").alias(\"id\"), \n    col(\"json.retweetCount\").alias(\"retweetCount\"),\n    col(\"json.lang\").alias(\"lang\"),\n    col(\"json.favoriteCount\").alias(\"favoriteCount\"),\n    col(\"json.user\").alias(\"user\"),\n    col(\"json.place.coordinates\").alias(\"coordinates\"),\n    col(\"json.place.name\").alias(\"name\"),\n    col(\"json.place.placeType\").alias(\"placeType\"),\n    col(\"json.place.fullName\").alias(\"fullName\"),\n    col(\"json.place.countryCode\").alias(\"countryCode\")   \n  )\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed37f595-39f2-42cd-88fe-4c4d6b4fb382"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 4.0. Display Twitter Data as a Table\nClick the left-most button in the bottom left corner."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6bac38d-205f-486f-b54b-45146845929a"}}},{"cell_type":"code","source":["cleanStream = \"SS99_clean_p\"\ndisplay(cleanDF, streamName = cleanStream)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc4dd947-ff1e-4cdc-b0dc-9ddd88f189b7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e1f6ea5-9583-4cb6-bcbd-79934a9eefb1"}}},{"cell_type":"code","source":["untilStreamIsReady(cleanStream)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48a0b04f-b2a4-4df1-aac7-03db03e56318"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When you are done, stop the stream:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6110a4e6-1ba2-426b-905f-29f1174ceeb0"}}},{"cell_type":"code","source":["for streamingQuery in spark.streams.active:\n  streamingQuery.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce9cecee-e2f7-4b32-9f6d-20fb9b782528"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 5.0. Process Hashtags\nIn this exercise, we do ETL processing on the `hashTags` column. The goal is to first convert hash tags all to lower case then group tweets and count by hash tags.  You will notice that `hashTags` is an array of hash tags, which you will have to break up (use `explode` function).  The `explode` method allows you to split an array column into multiple rows, copying all the other columns into each new row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41dd695a-4417-4408-a4ff-b7e042289864"}}},{"cell_type":"code","source":["from pyspark.sql.functions import explode, lower\n\ntwitCountsDF = (cleanDF                          # Start with \"cleanDF\"\n  .withColumn(\"hashTag\", explode(\"hashTags\"))    # Explode the array \"hashTags\" into \"hashTag\"\n  .withColumn(\"hashTag\", lower(col(\"hashTag\")))  # Convert \"hashTag\" to lower case\n  .groupBy(\"hashTag\")                            # Aggregate by \"hashTag\"\n  .count()                                       # For the aggregate, produce a count  \n  .orderBy(col(\"count\").desc())                  # Sort by \"count\"\n  .limit(25)                                     # Limit the result to 25 records\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2773ec18-66b8-4a42-b1b0-2e76f65b7857"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 6.0. Plot Counts of the Top 25 Most Popular Hashtags\nUnder **Plot Options**, use the following:\n* **Keys:** `hashTag`\n* **Values:** `count`\n\nIn **Display type**, use **Pie Chart** and click **Apply**.  Once you apply the plot options, be prepared to increase the size of the plot graphic using the resize widget in the lower right corner of the graphic area."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3419f41b-1b9b-4689-8d47-52ab1e55617b"}}},{"cell_type":"code","source":["twitStream = \"SS99_twit_p\"\ndisplay(twitCountsDF, streamName = twitStream)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"981559b0-a53e-4d10-84da-05c8174b22c0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e374602e-9f57-4545-b87b-620a7719714c"}}},{"cell_type":"code","source":["untilStreamIsReady(twitStream)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d3a0cc8-1a56-4389-8ab8-479969430eca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When you are done, stop the stream:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5126c13e-41cf-479f-8800-d6f098e705b8"}}},{"cell_type":"code","source":["for streamingQuery in spark.streams.active:\n  streamingQuery.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85e05cc2-8a1a-4f0b-b2a5-a2aa7fc6439a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 7.0. Read in File with Two-to-Three Letter Country Codes</h3>\nFor this next part we are going to take a look at the number of requests per country. To get started, we first need a lookup table that will give us the 3-character country code.\n1. Read in the file at `/mnt/training/countries/ISOCountryCodes/ISOCountryLookup.parquet`\n2. We will be interested in the `alpha2Code` and `alpha3Code` fields later"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c031882-997b-4055-90fc-304b594a50bd"}}},{"cell_type":"code","source":["countryCodeDF = spark.read.parquet(\"/mnt/training/countries/ISOCountryCodes/ISOCountryLookup.parquet\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a662c682-1fe4-45e5-9eee-b049d252c22d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 8.0. Join Tables &amp; Aggregate By Country\nIn `cleanDF`, there is a `countryCode` field. However, it is in the form of a two-letter country code.  The `display` map expects a three-letter country code.  In order to retrieve tweets with three-letter country codes, we will have to join `cleanDF` with `countryCodesDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b60e7199-a947-43b1-9a44-5c86b6731e75"}}},{"cell_type":"code","source":["mappedDF = (cleanDF\n  .filter(col(\"countryCode\").isNotNull())                                      # Filter out any nulls for \"countryCode\"\n  .join(countryCodeDF, cleanDF[\"countryCode\"] == countryCodeDF[\"alpha2Code\"])  # Join the two tables on \"countryCode\" and \"alpha2Code\"\n  .groupBy(col(\"alpha3Code\"))                                                  # Aggregate by country, \"alpha3Code\"\n  .count()                                                                     # Produce a count of each aggregate\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a32712a2-2b16-41ce-a3e3-1fdb280f0d16"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 9.0. Plot the Tweet Counts on a World Map\n\nUnder **Plot Options**, use the following:\n* **Keys:** `alpha3Code`\n* **Values:** `count`\n\nIn **Display type**, use **World map** and click **Apply**.\n\n<img src=\"https://files.training.databricks.com/images/eLearning/Structured-Streaming/plot-options-map-06.png\"/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88565270-d252-441f-8b15-0e4beee873c2"}}},{"cell_type":"code","source":["mappedStream = \"SS99_mapped_p\"\ndisplay(mappedDF, streamName = mappedStream)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc137e6f-2573-47c2-a915-a24893104afa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca8785ee-1155-4942-a1e1-90fa27ef92c8"}}},{"cell_type":"code","source":["untilStreamIsReady(mappedStream)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c65a45fb-531a-4923-ba8b-6352b75e7333"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 10.0. Write the stream to an in-memory table\n0. Use appropriate `format`\n0. For this exercise, we want to append new records to the results table\n0. Gives the query a name\n0. Start the query\n0. Assign the query to `mappedTablePython`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09b814f6-4c2b-433d-91eb-e7736c591afd"}}},{"cell_type":"code","source":["mappedQuery = (mappedDF \n .writeStream                           # From the DataFrame get the DataStreamWriter\n .format(\"memory\")                      # Specify the sink format as \"memory\"\n .outputMode(\"complete\")                # Configure the output mode as \"complete\"\n .queryName(\"mappedTablePython\")        # Name the query \"mappedTablePython\"\n .start()                               # Start the query\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac0c2df8-7e25-4dd0-bbdf-d73781e20364"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8712044b-da5d-4733-82f9-f4206847fb0e"}}},{"cell_type":"code","source":["untilStreamIsReady(\"mappedTablePython\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6fcbc0c-1a5c-4fed-9573-35468ac8da2a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Make sure to stop the stream before continuing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ddc539f-8dcf-4b4f-b3f8-9b5fd7d8d685"}}},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e60e2e2-8fdf-4868-97c1-d6bb838df0ee"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 11.0. Use SQL Syntax to Display a Few Rows</h2>\nAuthor a basic SQL query to display all columns, limiting the response to 10 rows.\nWe supplied `.queryName()` in the `writeStream` query above so that we would be able to treat `<queryName>` as a table for the sake of authoring SQL queries!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6834742c-8a05-459a-86f6-36e24b55c235"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM mappedTablePython LIMIT 10;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17b5f071-ddd7-4628-868f-adbbe137db65"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a88c385f-f95c-4fc6-8929-b45efc500b20"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c096c62d-407e-4342-828e-9c6765799dd5"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"04-Pulling-It-All-Together","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2730631136034164}},"nbformat":4,"nbformat_minor":0}
