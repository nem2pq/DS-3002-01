{"cells":[{"cell_type":"markdown","source":["### Structured Streaming with Kafka \nIn this notebook we'll examine how to connect Structured Streaming with Apache Kafka, a popular publish-subscribe system, to stream data from Wikipedia in real time, with a multitude of different languages. \n\n#### Objectives:\n* Learn About Kafka\n* Learn how to establish a connection with Kafka\n* Learn more aboutcreating visualizations\n\nFirst, run the following cell to import the data and make various utilities available for our experimentation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"337240cb-9edd-40f9-b25e-877af162b329"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acc476b4-e57d-47f0-9281-7216bd49c558"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1.0. The Kafka Ecosystem\nKafka is software designed upon the **publish/subscribe** messaging pattern.  Publish/subscribe messaging is where a sender (publisher) sends a message that is not specifically directed to any particular receiver (subscriber).  The publisher classifies the message somehow, and the receiver subscribes to receive certain categories of messages.  There are other usage patterns for Kafka, but this is the pattern we focus on in this course.\n\nPublisher/subscriber systems typically have a central point where messages are published, called a **broker**. The broker receives messages from publishers, assigns offsets to them and commits messages to storage.\n\nThe Kafka version of a unit of data is an array of bytes called a **message**. A message can also contain a bit of information related to partitioning called a **key**.  In Kafka, messages are categorized into **topics**.\n\n\n#### 1.1. The Kafka Server\nThe Kafka server is fed by a separate TCP server that reads the Wikipedia edits, in real time, from the various language-specific IRC channels to which Wikimedia posts them.  That server parses the IRC data, converts the results to JSON, and sends the JSON to a Kafka server, with the edits segregated by language. The various languages are **topics**.  For example, the Kafka topic \"en\" corresponds to edits for en.wikipedia.org.\n\n\n##### Required Options\nWhen consuming from a Kafka source, you **must** specify at least two options:\n1.  The Kafka bootstrap servers, for example: `dsr.option(\"kafka.bootstrap.servers\", \"server1.databricks.training:9092\")`\n2.  Some indication of the topics you want to consume."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38782785-9774-43c9-8fb6-6689b0709345"}}},{"cell_type":"markdown","source":["#### 1.2. Specifying a Topic\nThere are three, mutually-exclusive, ways to specify the topics for consumption:\n\n| Option        | Value                                          | Description                            | Example |\n| ------------- | ---------------------------------------------- | -------------------------------------- | ------- |\n| **subscribe** | A comma-separated list of topics               | A list of topics to which to subscribe | `dsr.option(\"subscribe\", \"topic1\")` <br/> `dsr.option(\"subscribe\", \"topic1,topic2,topic3\")` |\n| **assign**    | A JSON string indicating topics and partitions | Specific topic-partitions to consume.  | `dsr.dsr.option(\"assign\", \"{'topic1': [1,3], 'topic2': [2,5]}\")`\n| **subscribePattern**   | A (Java) regular expression           | A pattern to match desired topics      | `dsr.option(\"subscribePattern\", \"e[ns]\")` <br/> `dsr.option(\"subscribePattern\", \"topic[123]\")`|\n\n**Note:** In the example to follow, we're using the \"subscribe\" option to select the topics we're interested in consuming.  We've selected only the \"en\" topic, corresponding to edits for the English Wikipedia.  If we wanted to consume multiple topics (multiple Wikipedia languages, in our case), we could just specify them as a comma-separate list:\n\n```dsr.option(\"subscribe\", \"en,es,it,fr,de,eo\")```\n\nThere are other, optional, arguments you can give the Kafka source. For more information, see the <a href=\"https://people.apache.org//~pwendell/spark-nightly/spark-branch-2.1-docs/latest/structured-streaming-kafka-integration.html#\" target=\"_blank\">Structured Streaming and Kafka Integration Guide</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbbca507-bbd0-4eb9-a67a-1b99c8a3c38e"}}},{"cell_type":"markdown","source":["#### 1.3. The Kafka Schema\nReading from Kafka returns a `DataFrame` with the following fields:\n\n| Field             | Type   | Description |\n|------------------ | ------ |------------ |\n| **key**           | binary | The key of the record (not needed) |\n| **value**         | binary | Our JSON payload. We'll need to cast it to STRING |\n| **topic**         | string | The topic this record is received from (not needed) |\n| **partition**     | int    | The Kafka topic partition from which this record is received (not needed). This server only has one partition. |\n| **offset**        | long   | The position of this record in the corresponding Kafka topic partition (not needed) |\n| **timestamp**     | long   | The timestamp of this record  |\n| **timestampType** | int    | The timestamp type of a record (not needed) |\n\nIn the example below, the only column we want to keep is `value`.\n\n**Note:**  The default of `spark.sql.shuffle.partitions` is 200.  This setting is used in operations like `groupBy`. In this case, we should be setting this value to match the current number of cores."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbaddc81-e8b7-4b39-97f4-bb062e1c1874"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\nkafkaServer = \"server1.databricks.training:9092\"   # US (Oregon)\n# kafkaServer = \"server2.databricks.training:9092\" # Singapore\n\neditsDF = (spark.readStream                        # Get the DataStreamReader\n  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n  .option(\"kafka.bootstrap.servers\", kafkaServer)  # Configure the Kafka server name and port\n  .option(\"subscribe\", \"en\")                       # Subscribe to the \"en\" Kafka topic\n  .option(\"startingOffsets\", \"earliest\")           # Rewind stream to beginning when we restart notebook\n  .option(\"maxOffsetsPerTrigger\", 1000)            # Throttle Kafka's processing of the streams\n  .load()                                          # Load the DataFrame\n  .select(col(\"value\").cast(\"STRING\"))             # Cast the \"value\" column to STRING\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9f5cbaf-ab98-458a-94b1-2a60d495bc52"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's display some data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5204d306-c7c7-4cca-80c7-08936bb37106"}}},{"cell_type":"code","source":["myStreamName = \"lesson04a_ps\"\ndisplay(editsDF,  streamName = myStreamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75c79f85-51aa-48fc-8372-949f428a318f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9be54e0b-86eb-43bc-8d48-77f4861bf9d1"}}},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7ff95a3-33ea-4f73-9213-4b6f05a2c927"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Make sure to stop the stream before continuing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1f5ca72-26af-43c4-9ade-db536b3739cb"}}},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a87cd443-1a57-4c34-b24d-31984f8fde1d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2.0. Use Kafka to Display the Raw Data\n\nThe Kafka server acts as a sort of \"firehose\" (or asynchronous buffer) and displays raw data. Since raw data coming in from a stream is transient, we'd like to save it to a more permanent data structure.  The first step is to define the schema for the JSON payload.\n\n**Note:** Only those fields of future interest are commented below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e551a6be-4684-45fa-b34c-aee287d146da"}}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\nfrom pyspark.sql.functions import from_json, unix_timestamp\n\nschema = StructType([\n  StructField(\"channel\", StringType(), True),\n  StructField(\"comment\", StringType(), True),\n  StructField(\"delta\", IntegerType(), True),\n  StructField(\"flag\", StringType(), True),\n  StructField(\"geocoding\", StructType([                 # (OBJECT): Added by the server, field contains IP address geocoding information for anonymous edit.\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"countryCode2\", StringType(), True),\n    StructField(\"countryCode3\", StringType(), True),\n    StructField(\"stateProvince\", StringType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True),\n  ]), True),\n  StructField(\"isAnonymous\", BooleanType(), True),      # (BOOLEAN): Whether or not the change was made by an anonymous user\n  StructField(\"isNewPage\", BooleanType(), True),\n  StructField(\"isRobot\", BooleanType(), True),\n  StructField(\"isUnpatrolled\", BooleanType(), True),\n  StructField(\"namespace\", StringType(), True),         # (STRING): Page's namespace. See https://en.wikipedia.org/wiki/Wikipedia:Namespace \n  StructField(\"page\", StringType(), True),              # (STRING): Printable name of the page that was edited\n  StructField(\"pageURL\", StringType(), True),           # (STRING): URL of the page that was edited\n  StructField(\"timestamp\", StringType(), True),         # (STRING): Time the edit occurred, in ISO-8601 format\n  StructField(\"url\", StringType(), True),\n  StructField(\"user\", StringType(), True),              # (STRING): User who made the edit or the IP address associated with the anonymous editor\n  StructField(\"userURL\", StringType(), True),\n  StructField(\"wikipediaURL\", StringType(), True),\n  StructField(\"wikipedia\", StringType(), True),         # (STRING): Short name of the Wikipedia that was edited (e.g., \"en\" for the English)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64e07511-3b60-4843-bf33-445eefc1304f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next we can use the function `from_json` to parse out the full message with the schema specified above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49b998a9-a487-451d-8e89-6edbe113204d"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, from_json\n\njsonEdits = editsDF.select(\n  from_json(\"value\", schema).alias(\"json\"))  # Parse the column \"value\" and name it \"json\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a94e1053-cdd1-48b7-af74-190d696451b0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When parsing a value from JSON, we end up with a single column containing a complex object. We can clearly see this by simply printing the schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c7c1c4c-aa5e-42aa-90a6-adc2764ab1ff"}}},{"cell_type":"code","source":["jsonEdits.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95fe093b-12ec-4166-9245-bd9b670ec847"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The fields of a complex object can be referenced with a \"dot\" notation as in: `col(\"json.geocoding.countryCode3\")` \nSince a large number of these fields/columns can become unwieldy, it's common to extract the sub-fields and represent them as first-level columns as seen below:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd0cb056-74a6-46a9-8c0a-e7aecca62622"}}},{"cell_type":"code","source":["from pyspark.sql.functions import isnull, unix_timestamp\n\nanonDF = (jsonEdits\n  .select(col(\"json.wikipedia\").alias(\"wikipedia\"),      # Promoting from sub-field to column\n          col(\"json.isAnonymous\").alias(\"isAnonymous\"),  #     \"       \"      \"      \"    \"\n          col(\"json.namespace\").alias(\"namespace\"),      #     \"       \"      \"      \"    \"\n          col(\"json.page\").alias(\"page\"),                #     \"       \"      \"      \"    \"\n          col(\"json.pageURL\").alias(\"pageURL\"),          #     \"       \"      \"      \"    \"\n          col(\"json.geocoding\").alias(\"geocoding\"),      #     \"       \"      \"      \"    \"\n          col(\"json.user\").alias(\"user\"),                #     \"       \"      \"      \"    \"\n          col(\"json.timestamp\").cast(\"timestamp\"))       # Promoting and converting to a timestamp\n  .filter(col(\"namespace\") == \"article\")                 # Limit result to just articles\n  .filter(~isnull(col(\"geocoding.countryCode3\")))        # We only want results that are geocoded\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e44da8f1-6294-480c-a4b7-0bf2e4252490"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 2.1. Mapping Anonymous Editors' Locations\n\nWhen you run the query, the default is a [live] html table. The geocoded information allows us to associate an anonymous edit with a country. We can then use that geocoded information to plot edits on a [live] world map. In order to create a slick world map visualization of the data, you'll need to click on the item below.\n\nUnder **Plot Options**, use the following:\n* **Keys:** `countryCode3`\n* **Values:** `count`\n\nIn **Display type**, use **World map** and click **Apply**.\n\n<img src=\"https://files.training.databricks.com/images/eLearning/Structured-Streaming/plot-options-map-04.png\"/>\n\nBy invoking a `display` action on a DataFrame created from a `readStream` transformation, we can generate a LIVE visualization! \n\n**Note:** Keep an eye on the plot for a minute or two and watch the colors change."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b6d545c-cc78-4714-8cf6-9caf4c50b7cc"}}},{"cell_type":"code","source":["mappedDF = (anonDF\n  .groupBy(\"geocoding.countryCode3\") # Aggregate by country (code)\n  .count()                           # Produce a count of each aggregate\n)\ndisplay(mappedDF, streamName = myStreamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8df31d19-4e7e-4e4d-92ba-9fa172df9d7e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22f1d4a6-ad1f-4875-8960-894292e42e35"}}},{"cell_type":"code","source":["untilStreamIsReady(myStreamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c98aa186-9e8c-4530-bccf-c7377983575a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Stop the streams."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e439c65f-37c3-42c7-8399-5ba497a525d3"}}},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8190f24-f060-413b-87a9-1b735eab3839"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Review Questions\n\n**Q:** What `format` should you use with Kafka?<br>\n**A:** `format(\"kafka\")`\n\n**Q:** How do you specify a Kafka server?<br>\n**A:** `.option(\"kafka.bootstrap.servers\"\", \"server1.databricks.training:9092\")`\n\n**Q:** What verb should you use in conjunction with `readStream` and Kafka to start the streaming job?<br>\n**A:** `load()`, but with no parameters since we are pulling from a Kafka server.\n\n**Q:** What fields are returned in a Kafka DataFrame?<br>\n**A:** Reading from Kafka returns a DataFrame with the following fields:\nkey, value, topic, partition, offset, timestamp, timestampType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"924f4c27-c506-40bd-bc22-60799d95e2c3"}}},{"cell_type":"markdown","source":["Run the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5a65420-0837-4ff1-9e2c-372c3b72f711"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7216c15d-9e7a-480a-b055-bec5f04f0cc8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Additional Topics &amp; Resources\n\n* <a href=\"http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#creating-a-kafka-source-stream#\" target=\"_blank\">Create a Kafka Source Stream</a>\n* <a href=\"https://kafka.apache.org/documentation/\" target=\"_blank\">Official Kafka Documentation</a>\n* <a href=\"https://www.confluent.io/blog/okay-store-data-apache-kafka/\" target=\"_blank\">Use Kafka to store data</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4334ccfe-9b19-4235-9d55-b3693dc54f08"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"03-Using-Kafka","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2730631136034126}},"nbformat":4,"nbformat_minor":0}
