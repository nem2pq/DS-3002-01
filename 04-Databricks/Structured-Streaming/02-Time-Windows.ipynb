{"cells":[{"cell_type":"markdown","source":["### Working with Time Windows\nIn this notebook, we'll explore the problems associated with trying to aggregate streaming data, and then illustrate how to solve this problem using structures called windows, and by expiring old data using watermarking.\n\n#### Objectives:\n* Use sliding windows to aggregate over chunks of data rather than all data\n* Apply watermarking to throw away stale old data that you do not have space to keep\n* Plot live graphs using `display`\n\nFirst, run the following cell to import the data and make various utilities available for our experimentation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acd38de2-2b77-41e9-a590-3605fe30b697"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42835eca-233b-4a99-9965-7386d973c790"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Preparing the Python environment...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Preparing the Python environment..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Defining courseware-specific utility methods...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Defining courseware-specific utility methods..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Defining user-facing utility methods...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Defining user-facing utility methods..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Defining custom variables for this lesson...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Defining custom variables for this lesson..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Initializing Databricks Academy's testing framework...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Initializing Databricks Academy's testing framework..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Initializing Databricks Academy's services for generating dynamic data...","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Initializing Databricks Academy's services for generating dynamic data..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Mounting course-specific datasets to <b>/mnt/training</b>...</br>Datasets are already mounted to <b>/mnt/training</b> from <b>s3a://databricks-corp-training/common</b>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Mounting course-specific datasets to <b>/mnt/training</b>...</br>Datasets are already mounted to <b>/mnt/training</b> from <b>s3a://databricks-corp-training/common</b>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Cleaning up the learning environment...no actions taken.","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Cleaning up the learning environment...no actions taken."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"All done!","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["All done!"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Streaming Aggregations\nContinuous applications often require near real-time decisions on real-time, aggregated statistics.\n\nSome examples include: \n* Aggregating errors by type in data from IoT devices. \n* Detecting anomalous behavior by aggregating data by country from a server's log file(s). \n* Performing behavioral analysis on instant messages via hash tags.\n\nHowever, in the case of streams, you generally don't want to run aggregations over the entire dataset. \n<br>\n\n**But what happens if an attempt is made to aggregate over a stream's entire dataset?** <br>\n- While streams have a definitive start, conceptually there's no end to a stream; i.e., it's an [unbounded] data set.\n- Because there is no \"end\" to a stream, the size of the dataset grows in perpetuity.\n- Which means that your cluster would eventually run out of resources.\n\nSo, instead of aggregating over an entire dataset, streaming data must be aggregated over by grouping the data by *windows* of time (e.g.,, every 5 minutes, or every hour).  This technical approach is referred to as **windowing**.\n\n<br>\n\n### Windowing\nIf we were using a static DataFrame to produce an aggregate count, we could use `groupBy()` and `count()`.  However, we must insteadn accumulate counts within a **sliding window** in order to answer questions such as *\"How many records are we getting every second?\"*\n\nThe following illustration, from the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\" target=\"_blank\">Structured Streaming Programming Guide</a> guide, helps us understanding how it works:\n\n<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-window.png\" style=\"width: 900px;\">\n\n<br>\n\n#### Event Time *versus* Receipt Time\n- **Event Time** is the time at which the event occurred in the real world.\n- **Event Time** is **NOT** maintained by the Structured Streaming framework. \n\nAt best, Streams only knows about **Receipt Time** - the time a piece of data arrived in Spark.\n\n##### Examples of *Event Time*:\n* The timestamp recorded in each record of a log file\n* The instant at which an IoT device took a measurement\n* The moment a REST API received a request\n\n##### Examples of Receipt Time:\n- A timestamp added to a DataFrame the moment it was processed by Spark\n- The timestamp extracted from an hourly log file's file name\n- The time at which an IoT hub received a report of a device's measurement\n- Presumably offset by some delay from when the measurement was taken\n\nHowever, it should be born in mind that there are some problems inherent to using the **Receipt Time...** the main problem pertaining to accuracy.\n\nFor example, the time between when an IoT device takes a measurement versus when it is reported can be off by several minutes.  This may be of significant concern with regards to security and health devices. For example:\n- The timestamp embedded in an hourly log file can be off by up to one hour making correlations to other events extremely difficult\n- The timestamp added by Spark as part of a DataFrame transformation can be off by hours to weeks to months depending on when the event occurred and when the job ran\n\nSo then, are there situations where using **Receipt Time** rather than **Event Time** may be more appropriate?  Well, it depends... Receipt Time could be appropriate in circumstances where accuracy is not a significant concern (i.e., when **Receipt Time** is close enough to **Event Time**).\nFor example, where IoT events that could be delayed by minutes, but where the resolution of the query is greater (e.g., days or months)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51fc179f-ce3e-4346-9519-4669ce2c5a35"}}},{"cell_type":"markdown","source":["#### 1.0. Windowed Streaming\nThis exercise examines the files in `/mnt/training/sensor-data/accelerometer/time-series-stream.json/`.  Each line in this file contains a JSON record having two fields: `time` and `action`. Consider that new files will be written to this directory continuously (aka, streaming); therefore, conceptually there is no end to this process.\n\n###### 1.1. First, inspect the `head` of one of these files:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04ff38a7-1cfb-401f-a6da-7f6530783b69"}}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/sensor-data/accelerometer/time-series-stream.json/file-0.json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"866e16d9-ec88-412d-a823-369064ec1338"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###### 1.2. Next, analyze these files interactively. \nA schema must be specified for file-based Structured Streams; therefore, the first task is to define a schema for these files.\nDue to its simplicity, the schema can be defined using a simple DDL-formatted string representation of the schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2cfabd1-b916-471d-8aba-5abc36ecd456"}}},{"cell_type":"code","source":["inputPath = \"dbfs:/mnt/training/sensor-data/accelerometer/time-series-stream.json/\"\njsonSchema = \"time timestamp, action string\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0acbbead-7fbe-4cb2-ace6-f410cf4efd91"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###### 1.3. With the schema defined, create the initial DataFrame `inputDf`, and then `countsDF` to represent the aggregation:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7ef3ca4-4a73-42c8-8dc2-71d8da6abbfc"}}},{"cell_type":"code","source":["from pyspark.sql.functions import window, col\n\ninputDF = (spark\n  .readStream                                 # Returns an instance of DataStreamReader\n  .schema(jsonSchema)                         # Set the schema of the JSON data\n  .option(\"maxFilesPerTrigger\", 1)            # Treat a sequence of files as a stream, one file at a time\n  .json(inputPath)                            # Specifies the format, path and returns a DataFrame\n)\n\ncountsDF = (inputDF\n  .groupBy(col(\"action\"),                     # Aggregate by action...\n           window(col(\"time\"), \"1 hour\"))     # ...then by a 1 hour window\n  .count()                                    # For the aggregate, produce a count\n  .select(col(\"window.start\").alias(\"start\"), # Elevate field to column\n          col(\"count\"),                       # Include count\n          col(\"action\"))                      # Include action\n  .orderBy(col(\"start\"))                      # Sort by the start time\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a465409-33fc-4097-910c-bfa325b5c327"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["###### 1.4. To view the results of the query, pass the DataFrame `countsDF` to the `display()` function.\nAs in the previous lesson, specify the stream's `streamName` to gain better control of it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b3158d9-42dd-4845-b4b2-786c6c1092d0"}}},{"cell_type":"code","source":["streamName = \"lesson03_ps\"\ndisplay(countsDF,  streamName = streamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d184df8-880a-4d49-9391-015be804ef7a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 2.0. Performance Considerations\nIf you run that query, as is, it will take a surprisingly long time to start generating data. What's the cause of the delay? If you expand the **Spark Jobs** component, you'll see something like this:\n\n<img src=\"https://files.training.databricks.com/images/structured-streaming-shuffle-partitions-200.png\"/>\n\n<br>\n\nIt's our `groupBy()`. `groupBy()` causes a _shuffle_, and, by default, Spark SQL shuffles to 200 partitions. In addition, we're doing a _stateful_ aggregation: one that requires Structured Streaming to maintain and aggregate data over time.\n\nWhen doing a stateful aggregation, Structured Streaming must maintain an in-memory _state map_ for each window within each partition. For fault tolerance reasons, the state map has to be saved after a partition is processed, and it needs to be saved somewhere fault-tolerant. To meet those requirements, the Streaming API saves the maps to a distributed store. On some clusters, that will be HDFS. Databricks uses the DBFS.\n\nThat means that every time it finishes processing a window, the Streaming API writes its internal map to disk. The write has some overhead, typically between 1 and 2 seconds."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7b25017-a7a2-456c-bd88-2d25b6822941"}}},{"cell_type":"code","source":["untilStreamIsReady(streamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2bbbda4-4b75-4a9f-a82c-fd8481bd211b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Before proceeding, we need to stop any streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4c85100-13af-4aee-a1d2-fe3c466436ee"}}},{"cell_type":"code","source":["# for s in spark.streams.active: # Iterate over all active streams\n#   s.stop()                     # Stop the stream\n\n# As mentioned in lesson #2, we have provided additional methods for working with streams, and in  \n# this case, for dealing with the rare exceptions that may arise as a result of terminating a stream.\n# Listed above is the logical equivalent to this operation.\nstopAllStreams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20075051-7521-4aab-a0fb-80e6a525bf79"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["One way to reduce this overhead is to reduce the number of partitions Spark shuffles to. In most cases, you want a 1-to-1 mapping of partitions to cores for streaming applications.  Rerun the query below and notice the performance improvement.\n\nOnce the data is loaded, render a line graph with \n* **Keys** is set to `start`\n* **Series groupings** is set to `action`\n* **Values** is set to `count`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"984dd4f4-d367-4d68-82b7-37d6cc66489b"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\ndisplay(countsDF,  streamName = streamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5469f0de-8e1b-45ac-b350-1449fabb29f7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wait until stream is done initializing..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aa2d6c0-d8fa-4559-90b1-debd03ff0ff7"}}},{"cell_type":"code","source":["untilStreamIsReady(streamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96192d89-0381-4295-a8f7-cf7fa2e00e99"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When you are done, stop all the streaming jobs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0518cf3-44ae-4f15-883e-4ed97c3804e1"}}},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fa12dff-6f5a-4f64-a613-f606eee65ea3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### 2.1. Problem with Generating Many Windows\nWe are generating a window for every 1 hour aggregate.  _Every window_ has to be separately persisted and maintained. Over time, this aggregated data will build up in the driver. The end result being a massive slowdown if not an OOM Error.\n\n###### How do we fix that problem?\nOne simple solution is to increase the size of our window (say, to 2 hours). That way, we're generating fewer windows.\nBut if the job runs for a long time, we're still building up an unbounded set of windows. Eventually, we could hit resource limits.\n\n<br>\n\n#### 3.0. Watermarking\nA better solution to the problem is to define a cut-off (i.e., a point after which Structured Streaming is allowed to throw saved windows away). That's what _watermarking_ allows us to do.  Below is our previous example with watermarking. Structured Streaming is instructed to keep no more than 2 hours of aggregated data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"270a342e-b747-4d00-be18-9212e4033d56"}}},{"cell_type":"code","source":["watermarkedDF = (inputDF\n  .withWatermark(\"time\", \"2 hours\")             # Specify a 2-hour watermark\n  .groupBy(col(\"action\"),                       # Aggregate by action...\n           window(col(\"time\"), \"1 hour\"))       # ...then by a 1 hour window\n  .count()                                      # For each aggregate, produce a count\n  .select(col(\"window.start\").alias(\"start\"),   # Elevate field to column\n          col(\"count\"),                         # Include count\n          col(\"action\"))                        # Include action\n  .orderBy(col(\"start\"))                        # Sort by the start time\n)\ndisplay(watermarkedDF, streamName = streamName) # Start the stream and display it"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21f41e02-6c60-4a26-953f-80c360d4b8e7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In the example above,   \n* Data received 2 hour _past_ the watermark will be dropped. \n* Data received within 2 hours of the watermark will never be dropped.\n\nMore specifically, any data less than 2 hours behind the latest data processed till then is guaranteed to be aggregated. However, the guarantee is strict only in one direction. Data delayed by more than 2 hours is not guaranteed to be dropped; it may or may not get aggregated. The more delayed the data is, the less likely the engine is going to process it.\n\n**Wait until stream is done initializing...**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4755fbda-fdbb-4876-90bd-fe576fcc7c4f"}}},{"cell_type":"code","source":["untilStreamIsReady(streamName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8ed51aa-201b-4ce1-9c9a-a3eb567c2c61"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Stop all the streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c6af38a-1784-4ccd-99e5-8a3e2bca6f32"}}},{"cell_type":"code","source":["stopAllStreams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03dc65f5-668e-4ab0-ac34-2d8e641d6539"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Run the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2372d763-10d8-4a17-a7e4-3e760c8ea2b9"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6864413d-2911-425b-80f2-75a2d9c873e5"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02-Time-Windows","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1430281655517795}},"nbformat":4,"nbformat_minor":0}
